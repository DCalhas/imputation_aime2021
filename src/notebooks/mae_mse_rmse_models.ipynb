{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from multiprocessing import Process, Manager, Semaphore\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import GPyOpt\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils.losses_utils as losses\n",
    "\n",
    "import utils.data_utils as data_utils\n",
    "\n",
    "import utils.fmri_utils as fmri_utils\n",
    "\n",
    "import utils.imputation_utils as imputation\n",
    "\n",
    "import utils.training_utils as training\n",
    "\n",
    "import utils.layers_utils as layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "home = str(Path.home())\n",
    "\n",
    "dataset=\"02\"#\"01\", \"02\"\n",
    "\n",
    "bold_shift=3\n",
    "f_resample=1.8\n",
    "fmri_resolution_factor=6\n",
    "n_partitions=24\n",
    "by_partitions=False\n",
    "partition_length=14\n",
    "n_voxels = 100\n",
    "missing_values_mode = \"region\"#\"region\", \"time\", \"volume\"\n",
    "rm_rate = 0.10\n",
    "\n",
    "cpu_number = multiprocessing.cpu_count()\n",
    "cpu_usage = 0.50\n",
    "concurrency = int(cpu_number*cpu_usage)\n",
    "\n",
    "device = \"CPU\"\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices(device)[0]\n",
    "\n",
    "if(device == \"CPU\"):\n",
    "    devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "    tf.config.experimental.set_visible_devices(devices=devices, device_type='CPU')\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "    tf.config.log_device_placement=True\n",
    "    \n",
    "else:\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "    tf.config.log_device_placement=True\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpu,\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 49, 111, 14)\n",
      "(196, 100, 14)\n"
     ]
    }
   ],
   "source": [
    "if(dataset==\"01\"):\n",
    "    instances = list(range(10))\n",
    "elif(dataset==\"02\"):\n",
    "    instances = list(range(14))\n",
    "    \n",
    "_, _bold_set, mask, scalers = data_utils.load_data(instances, \n",
    "                                                    n_voxels=None, \n",
    "                                                    bold_shift=bold_shift, \n",
    "                                                    n_partitions=n_partitions, \n",
    "                                                    by_partitions=by_partitions, \n",
    "                                                    partition_length=partition_length, \n",
    "                                                    f_resample=f_resample, \n",
    "                                                    fmri_resolution_factor=fmri_resolution_factor, \n",
    "                                                    task=1, #1-auditory;2-visual\n",
    "                                                    run=1,\n",
    "                                                    standardize_eeg=True, \n",
    "                                                    standardize_fmri=True, \n",
    "                                                    dataset=dataset)\n",
    "\n",
    "coords = np.argwhere(mask.mask_img_.get_data() == 1.0)\n",
    "\n",
    "if(dataset==\"01\"):\n",
    "    n_individuals=10\n",
    "    n_individuals_train = 8\n",
    "    n_individuals_test = 2\n",
    "\n",
    "    n_frames = _bold_set.shape[2]\n",
    "\n",
    "    _bold_set = _bold_set[:,:n_voxels]\n",
    "    _bold_set = _bold_set.reshape(_bold_set.shape + (1,))\n",
    "    \n",
    "    min_train = np.amin(_bold_set)\n",
    "    max_value = np.amin([min_train])\n",
    "    _bold_set = _bold_set - max_value+0.001\n",
    "\n",
    "    _bold_set = np.log(_bold_set)\n",
    "    _bold_set = _bold_set.astype('float32')\n",
    "    \n",
    "    coords = coords[:n_voxels]\n",
    "    \n",
    "elif(dataset==\"02\"):\n",
    "    n_individuals=14\n",
    "    \n",
    "    n_individuals_train = 10\n",
    "    n_individuals_test = 4\n",
    "    \n",
    "    n_frames = _bold_set.shape[2]\n",
    "    \n",
    "    _bold_set = _bold_set.reshape(_bold_set.shape + (1,))\n",
    "    _bold_set = _bold_set[:,:n_voxels]\n",
    "    _bold_set = _bold_set.astype('float32')\n",
    "    \n",
    "    coords = coords[:n_voxels]\n",
    "    \n",
    "if(not by_partitions):\n",
    "    n_partitions=int(_bold_set.shape[0]/n_individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_train = _bold_set[:n_partitions*n_individuals_train]\n",
    "bold_test = _bold_set[n_partitions*n_individuals_train:]\n",
    "\n",
    "bold_train = data_utils.scale_voxels(bold_train)\n",
    "bold_test = data_utils.scale_voxels(bold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_corr_matrix = imputation.compute_corr_matrix(bold_train, n_individuals=n_individuals_train, n_partitions=n_partitions)\n",
    "train_corr_matrix = train_corr_matrix[:n_voxels,:n_voxels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_bold_train = data_utils.shuffle_individuals_batch(bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train, n_frames=n_frames)\n",
    "shuffled_bold_test = data_utils.shuffle_individuals_batch(bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test, n_frames=n_frames)\n",
    "\n",
    "shuffled_bold_train = data_utils.decompress_maintain_dim(shuffled_bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train)\n",
    "shuffled_bold_test = data_utils.decompress_maintain_dim(shuffled_bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Remove Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(missing_values_mode == \"volume\"):\n",
    "    missing_values_bold_train = imputation.random_missing_values_volume(shuffled_bold_train, rm_pct=rm_rate)\n",
    "    missing_values_bold_test = imputation.random_missing_values_volume(shuffled_bold_test, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_train = imputation.random_missing_values_volume(bold_train, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_test = imputation.random_missing_values_volume(bold_test, rm_pct=rm_rate)\n",
    "elif(missing_values_mode == \"time\"):\n",
    "    missing_values_bold_train = imputation.random_missing_values_time(shuffled_bold_train, rm_pct=rm_rate)\n",
    "    missing_values_bold_test = imputation.random_missing_values_time(shuffled_bold_test, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_train = imputation.random_missing_values_time(bold_train, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_test = imputation.random_missing_values_time(bold_test, rm_pct=rm_rate)\n",
    "elif(missing_values_mode == \"region\"):\n",
    "    missing_values_bold_train = imputation.region_missing_values_time(shuffled_bold_train, coords, rm_pct=rm_rate)    \n",
    "    missing_values_bold_test = imputation.region_missing_values_time(shuffled_bold_test, coords, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_train = imputation.region_missing_values_time(bold_train, coords, rm_pct=rm_rate)\n",
    "    denoising_missing_values_bold_test = imputation.region_missing_values_time(bold_test, coords, rm_pct=rm_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Masks and reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_bold_train = data_utils.compress_maintain_dim(missing_values_bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train)\n",
    "missing_values_bold_test = data_utils.compress_maintain_dim(missing_values_bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "denoising_missing_values_bold_train = data_utils.compress_maintain_dim(denoising_missing_values_bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train)\n",
    "denoising_missing_values_bold_test = data_utils.compress_maintain_dim(denoising_missing_values_bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "\n",
    "bold_missing_with_mask_train = imputation.dataset_mask_missing_values(missing_values_bold_train)\n",
    "bold_missing_with_mask_test = imputation.dataset_mask_missing_values(missing_values_bold_test)\n",
    "target_missing_train = data_utils.compress_maintain_dim(shuffled_bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train)\n",
    "target_missing_test = data_utils.compress_maintain_dim(shuffled_bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "denoising_bold_missing_with_mask_train = imputation.dataset_mask_missing_values(denoising_missing_values_bold_train)\n",
    "denoising_bold_missing_with_mask_test = imputation.dataset_mask_missing_values(denoising_missing_values_bold_test)\n",
    "target_bold_train = data_utils.compress_maintain_dim(bold_train, n_partitions=n_partitions, n_individuals=n_individuals_train)\n",
    "target_bold_test = data_utils.compress_maintain_dim(bold_test, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "\n",
    "bold_missing_with_mask_train = bold_missing_with_mask_train.astype('float32')\n",
    "bold_missing_with_mask_test = bold_missing_with_mask_test.astype('float32')\n",
    "target_missing_train = target_missing_train.astype('float32')\n",
    "target_missing_test = target_missing_test.astype('float32')\n",
    "denoising_bold_missing_with_mask_train = denoising_bold_missing_with_mask_train.astype('float32')\n",
    "denoising_bold_missing_with_mask_test = denoising_bold_missing_with_mask_test.astype('float32')\n",
    "target_bold_train = target_bold_train.astype('float32')\n",
    "target_bold_test = target_bold_test.astype('float32')\n",
    "\n",
    "target_missing_train = target_missing_train.reshape(target_missing_train.shape[:-1])\n",
    "target_missing_test = target_missing_test.reshape(target_missing_test.shape[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built Iterative only Spatial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = np.load(home+\"/fmri_imputation/hyperparameters/01_chained_spatial_only_hyperparameters.npy\")\n",
    "\n",
    "current_epochs_chained_spatial_only=int(hyperparameters[0])\n",
    "current_learning_rate=float(hyperparameters[1])\n",
    "current_batch_size=14\n",
    "current_optimizer_chained_spatial_only = tf.keras.optimizers.Adamax(learning_rate=current_learning_rate)\n",
    "\n",
    "chained_spatial_only = layers.RecursiveMLPBlock(target_missing_train.shape[1], corr_matrix=train_corr_matrix, activation=tf.keras.activations.linear)\n",
    "chained_spatial_only.build(input_shape=(1, target_missing_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built Dropout only Spatial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = np.load(home+\"/fmri_imputation/hyperparameters/01_dropout_spatial_only_hyperparameters.npy\")\n",
    "\n",
    "current_epochs_dropout_spatial_only=int(hyperparameters[0])\n",
    "current_learning_rate=float(hyperparameters[1])\n",
    "current_batch_size=14\n",
    "current_optimizer_dropout_spatial_only = tf.keras.optimizers.Adamax(learning_rate=current_learning_rate)\n",
    "\n",
    "dropout_spatial_only = layers.DropoutMLPBlock(target_missing_train.shape[1], activation=tf.keras.activations.linear)\n",
    "dropout_spatial_only.build(input_shape=(1, target_missing_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Iterative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funtion = tf.keras.activations.linear\n",
    "\n",
    "hyperparameters = np.load(home+\"/fmri_imputation/hyperparameters/01_chained_joint_hyperparameters.npy\")\n",
    "\n",
    "current_epochs_spatial_chained=int(hyperparameters[0])\n",
    "current_epochs_denoiser_chained=int(hyperparameters[1])\n",
    "current_epochs_iterations_chained=int(hyperparameters[2])\n",
    "current_learning_rate_spatial_chained=float(hyperparameters[3])\n",
    "current_learning_rate_denoiser_chained=float(hyperparameters[4])\n",
    "current_denoiser_reg_chained=float(hyperparameters[5])\n",
    "current_use_bias_chained=bool(hyperparameters[6])\n",
    "current_dropout_chained=float(hyperparameters[7])\n",
    "current_recurrent_dropout_chained=float(hyperparameters[8])\n",
    "current_batch_size=14\n",
    "current_optimizer_spatial_chained = tf.keras.optimizers.Adamax(learning_rate=current_learning_rate_spatial_chained)\n",
    "current_optimizer_denoiser_chained = tf.keras.optimizers.Adam(learning_rate=current_learning_rate_denoiser_chained)\n",
    "\n",
    "iterative = layers.RecursiveMLPBlock(target_missing_train.shape[1], corr_matrix=train_corr_matrix, activation=activation_funtion)\n",
    "iterative.build(input_shape=(1, target_missing_train.shape[1]))\n",
    "\n",
    "denoiser_chained = tf.keras.Sequential()\n",
    "denoiser_chained.add(tf.keras.layers.GRU(1, kernel_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_chained),\n",
    "                                    recurrent_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_chained),\n",
    "                                    bias_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_chained),\n",
    "                                    return_sequences=True, input_shape=_bold_set.shape[2:],\n",
    "                                    use_bias=current_use_bias_chained, dropout=current_dropout_chained, \n",
    "                                    recurrent_dropout=current_recurrent_dropout_chained))\n",
    "denoiser_chained.build(input_shape=_bold_set.shape[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dropout Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funtion = tf.keras.activations.linear\n",
    "\n",
    "hyperparameters = np.load(home+\"/fmri_imputation/hyperparameters/01_dropout_joint_hyperparameters.npy\")\n",
    "\n",
    "current_epochs_spatial_dropout=int(hyperparameters[0])\n",
    "current_epochs_denoiser_dropout=int(hyperparameters[1])\n",
    "current_epochs_iterations_dropout=int(hyperparameters[2])\n",
    "current_learning_rate_spatial_dropout=float(hyperparameters[3])\n",
    "current_learning_rate_denoiser_dropout=float(hyperparameters[4])\n",
    "current_denoiser_reg_dropout=float(hyperparameters[5])\n",
    "current_use_bias_dropout=bool(hyperparameters[6])\n",
    "current_dropout_dropout=float(hyperparameters[7])\n",
    "current_recurrent_dropout_dropout=float(hyperparameters[8])\n",
    "current_batch_size=14\n",
    "current_optimizer_spatial_dropout = tf.keras.optimizers.Adamax(learning_rate=current_learning_rate_spatial_dropout)\n",
    "current_optimizer_denoiser_dropout = tf.keras.optimizers.Adam(learning_rate=current_learning_rate_denoiser_dropout)\n",
    "\n",
    "dropout = layers.DropoutMLPBlock(target_missing_train.shape[1], activation=activation_funtion)\n",
    "dropout.build(input_shape=(1, target_missing_train.shape[1]))\n",
    "\n",
    "denoiser_dropout = tf.keras.Sequential()\n",
    "denoiser_dropout.add(tf.keras.layers.GRU(1, kernel_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_dropout),\n",
    "                                    recurrent_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_dropout),\n",
    "                                    bias_regularizer=tf.keras.regularizers.l2(current_denoiser_reg_dropout),\n",
    "                                    return_sequences=True, input_shape=_bold_set.shape[2:],\n",
    "                                    use_bias=current_use_bias_dropout, dropout=current_dropout_dropout, \n",
    "                                    recurrent_dropout=current_recurrent_dropout_dropout))\n",
    "denoiser_dropout.build(input_shape=_bold_set.shape[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Context Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ce_bold_missing_with_mask_train = bold_missing_with_mask_train.reshape(bold_missing_with_mask_train.shape + (1,))\n",
    "#ce_target_missing_train = target_missing_train.reshape(target_missing_train.shape + (1,))\n",
    "\n",
    "#ce_bold_missing_with_mask_test = bold_missing_with_mask_test.reshape(bold_missing_with_mask_test.shape + (1,))\n",
    "#ce_target_missing_test = target_missing_test.reshape(target_missing_test.shape + (1,))\n",
    "\n",
    "#ce_model = tf.keras.Sequential()\n",
    "\n",
    "#ce_model.add(tf.keras.layers.Conv1D(1, kernel_size=(10), strides=(2), input_shape=ce_target_missing_train.shape[1:]))\n",
    "#ce_model.add(tf.keras.layers.UpSampling1D(size=5.0))\n",
    "#ce_model.add(tf.keras.layers.Conv1D(1, kernel_size=(31), strides=(2)))\n",
    "\n",
    "#ce_model.build(input_shape=ce_target_missing_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Mean Imputation from Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputation = imputation.Mean_Imputation(bold_missing_with_mask_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build kNN Imputation from Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = imputation.knn_imputation(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss Retrieval Train: 0.8716230311564037\n",
      "Epoch 1 Average Loss Complete Train: 0.9127404532262258\n",
      "Epoch 2 Average Loss Retrieval Train: 0.8348909910236086\n",
      "Epoch 2 Average Loss Complete Train: 0.889430022239685\n",
      "Epoch 1 Average Loss Retrieval Train: 0.8738270027296884\n",
      "Epoch 1 Average Loss Complete Train: 0.872578547682081\n",
      "Epoch 2 Average Loss Retrieval Train: 0.8401124664715358\n",
      "Epoch 2 Average Loss Complete Train: 0.8549553449664797\n",
      "Epoch 1 Average Loss Retrieval Train: 0.8670862061636788\n",
      "Epoch 1 Average Loss Complete Train: 0.9031357880149569\n",
      "Epoch 2 Average Loss Retrieval Train: 0.8078877440520695\n",
      "Epoch 2 Average Loss Complete Train: 0.8674484418971198\n",
      "Epoch 3 Average Loss Retrieval Train: 0.7709479868412018\n",
      "Epoch 3 Average Loss Complete Train: 0.8502533346414566\n",
      "Epoch 4 Average Loss Retrieval Train: 0.7428950079849788\n",
      "Epoch 4 Average Loss Complete Train: 0.8391434354441506\n",
      "Epoch 1  with train loss: 0.8146643924713135\n",
      "Epoch 2  with train loss: 0.8035518300533294\n",
      "Epoch 1 Average Loss Retrieval Train: 0.72117797264031\n",
      "Epoch 1 Average Loss Complete Train: 0.832151426587786\n",
      "Epoch 2 Average Loss Retrieval Train: 0.7040483717407499\n",
      "Epoch 2 Average Loss Complete Train: 0.8278060278722218\n",
      "Epoch 3 Average Loss Retrieval Train: 0.6905113705566951\n",
      "Epoch 3 Average Loss Complete Train: 0.8252254430736814\n",
      "Epoch 4 Average Loss Retrieval Train: 0.6794893762895039\n",
      "Epoch 4 Average Loss Complete Train: 0.8238185473850795\n",
      "Epoch 1  with train loss: 0.8029445576667785\n",
      "Epoch 2  with train loss: 0.8029393482208252\n",
      "Epoch 1 Average Loss Retrieval Train: 0.8545166679791042\n",
      "Epoch 1 Average Loss Complete Train: 0.856206750869751\n",
      "Epoch 2 Average Loss Retrieval Train: 0.7681925709758486\n",
      "Epoch 2 Average Loss Complete Train: 0.8169689591441835\n",
      "Epoch 3 Average Loss Retrieval Train: 0.7253547600337438\n",
      "Epoch 3 Average Loss Complete Train: 0.8059576038803373\n",
      "Epoch 1  with train loss: 0.8029782325029373\n",
      "Epoch 2  with train loss: 0.8029569107294082\n",
      "Epoch 3  with train loss: 0.8029615777730942\n",
      "Epoch 4  with train loss: 0.802965766787529\n",
      "Epoch 1 Average Loss Retrieval Train: 0.6973030030727386\n",
      "Epoch 1 Average Loss Complete Train: 0.8010053157806396\n",
      "Epoch 2 Average Loss Retrieval Train: 0.6776951921837671\n",
      "Epoch 2 Average Loss Complete Train: 0.7989145870719637\n",
      "Epoch 3 Average Loss Retrieval Train: 0.6629076825720923\n",
      "Epoch 3 Average Loss Complete Train: 0.7981914294617517\n",
      "Epoch 1  with train loss: 0.8029619961977005\n",
      "Epoch 2  with train loss: 0.8029645103216171\n",
      "Epoch 3  with train loss: 0.8029583066701889\n",
      "Epoch 4  with train loss: 0.8029589086771012\n"
     ]
    }
   ],
   "source": [
    "training.imputation_training(chained_spatial_only, bold_missing_with_mask_train, target_missing_train, missing_mask_val=None, target_missing_val=None,optimizer=current_optimizer_chained_spatial_only, batch_size=current_batch_size, n_epochs=current_epochs_chained_spatial_only, verbose=True)\n",
    "\n",
    "training.imputation_training(dropout_spatial_only, bold_missing_with_mask_train, target_missing_train, missing_mask_val=None, target_missing_val=None,optimizer=current_optimizer_dropout_spatial_only, batch_size=current_batch_size, n_epochs=current_epochs_dropout_spatial_only, verbose=True)\n",
    "\n",
    "training.alternate_training_spatial_denoiser(iterative, denoiser_chained, bold_missing_with_mask_train, target_missing_train, None, None, denoising_bold_missing_with_mask_train, None, target_bold_train, None,epochs_iterations=current_epochs_iterations_chained, optimizer_spatial=current_optimizer_spatial_chained, optimizer_denoiser=current_optimizer_denoiser_chained, batch_size=current_batch_size, epochs_spatial=current_epochs_spatial_chained, epochs_denoiser=current_epochs_denoiser_chained, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals_train=n_individuals_train,verbose=True)\n",
    "\n",
    "training.alternate_training_spatial_denoiser(dropout, denoiser_dropout, bold_missing_with_mask_train, target_missing_train, None, None, denoising_bold_missing_with_mask_train, None, target_bold_train, None,epochs_iterations=current_epochs_iterations_dropout, optimizer_spatial=current_optimizer_spatial_dropout, optimizer_denoiser=current_optimizer_denoiser_dropout, batch_size=current_batch_size, epochs_spatial=current_epochs_spatial_dropout, epochs_denoiser=current_epochs_denoiser_dropout, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals_train=n_individuals_train,verbose=True)\n",
    "\n",
    "#ce_results = training.ce_training(ce_model, ce_bold_missing_with_mask_train, ce_target_missing_train, optimizer=tf.keras.optimizers.Adamax(learning_rate=0.0001), batch_size=14, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Outputs from Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_chained_spatial_only_test = data_utils.out_by_batch(chained_spatial_only, bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels)\n",
    "out_dropout_spatial_only_test = data_utils.out_by_batch(dropout_spatial_only, bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels)\n",
    "\n",
    "imputted_bold_test, target_denoised_test = data_utils.format_to_denoiser(data_utils.out_by_batch(dropout, denoising_bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels), denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "dropout_denoised_test, dropout_target_test = data_utils.format_to_mae_loss(denoiser_dropout(imputted_bold_test).numpy(), target_denoised_test, n_individuals=n_individuals_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions)\n",
    "\n",
    "imputted_bold_test, target_denoised_test = data_utils.format_to_denoiser(data_utils.out_by_batch(iterative, denoising_bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels), denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "chained_denoised_test, chained_target_test = data_utils.format_to_mae_loss(denoiser_chained(imputted_bold_test).numpy(), target_denoised_test, n_individuals=n_individuals_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions)\n",
    "\n",
    "#out_ce_train = ce_model(ce_bold_missing_with_mask_train[:,0,:]).numpy()\n",
    "#out_ce_test = ce_model(ce_bold_missing_with_mask_test[:,0,:]).numpy()\n",
    "\n",
    "#out_ce_train = out_ce_train.reshape(out_ce_train.shape[:-1])\n",
    "#out_ce_test = out_ce_test.reshape(out_ce_test.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_utils.deshuflle_individuals_batch(chained_denoised_test, \n",
    "                                n_individuals=n_individuals_test,\n",
    "                                n_partitions=n_partitions,\n",
    "                                n_frames=_bold_set.shape[2])\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "view_imputted_bold = bold_missing_with_mask_test[:,0,:].copy()\n",
    "\n",
    "for instance in range(x.shape[0]):\n",
    "    \n",
    "    i = 0\n",
    "    #print(view_imputted_bold[instance])\n",
    "    print(np.sum(bold_missing_with_mask_test[instance, 1, :]))\n",
    "    for voxel in range(bold_missing_with_mask_test.shape[2]):\n",
    "        if(bold_missing_with_mask_test[instance,1,voxel]):\n",
    "            #print(i)\n",
    "            #view_imputted_bold[instance, voxel] = x[instance, i]\n",
    "            i+=1\n",
    "    #print(view_imputted_bold[instance])\n",
    "    \n",
    "    \n",
    "x = data_utils.decompress_maintain_dim(x.reshape(x.shape +(1,)), n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "print(x.shape)\n",
    "\n",
    "print(bold_missing_with_mask_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mean_train = mean_imputation.imputation_batch(bold_missing_with_mask_train)\n",
    "out_mean_test = mean_imputation.imputation_batch(bold_missing_with_mask_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute kNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_knn_test = knn.imputation_batch(bold_missing_with_mask_test, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Barycenter Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_barycenter_test = imputation.barycenter_imputation(imputation.random_missing_values_time(shuffled_bold_test, rm_pct=rm_rate), n_partitions=n_partitions, n_individuals=n_individuals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute MICE Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mice_test = imputation.mice_imputation(bold_missing_with_mask_train, bold_missing_with_mask_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN $0.84\\\\pm2.61$\n",
      "Barycenter $1.02\\\\pm3.16$\n",
      "mean $0.81\\\\pm2.50$\n",
      "MICE $0.82\\\\pm2.53$\n",
      "Dropout Spatial Only $10.71\\\\pm30.60$\n",
      "dropout $0.82\\\\pm0.60$\n",
      "Chained Spatial Only $10.71\\\\pm30.60$\n",
      "iterative $0.83\\\\pm0.60$\n"
     ]
    }
   ],
   "source": [
    "loss_knn_val = losses.compute_mae_loss(out_knn_test, target_missing_test, bold_missing_with_mask_test, model_name=\"kNN\", set_name=\"test\")\n",
    "\n",
    "loss_barycenter_val = losses.compute_mae_loss(out_barycenter_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Barycenter\", set_name=\"test\")\n",
    "\n",
    "loss_mean_val = losses.compute_mae_loss(out_mean_test, target_missing_test, bold_missing_with_mask_test, model_name=\"mean\", set_name=\"test\")\n",
    "\n",
    "#loss_ce_val = losses.compute_mae_loss(out_ce_test, target_missing_test, bold_missing_with_mask_test, model_name=\"ce\", set_name=\"test\")\n",
    "\n",
    "loss_mice_val = losses.compute_mae_loss(out_mice_test, target_missing_test, bold_missing_with_mask_test, model_name=\"MICE\", set_name=\"test\")\n",
    "\n",
    "loss_dropout_spatial_val = losses.compute_mae_loss(out_dropout_spatial_only_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Dropout Spatial Only\", set_name=\"test\")\n",
    "\n",
    "loss_dropout_val = losses.compute_mae_loss_maskless(dropout_denoised_test, dropout_target_test, model_name=\"dropout\", set_name=\"test\")\n",
    "\n",
    "loss_chained_spatial_val = losses.compute_mae_loss(out_chained_spatial_only_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Chained Spatial Only\", set_name=\"test\")\n",
    "\n",
    "loss_iterative_val = losses.compute_mae_loss_maskless(chained_denoised_test, chained_target_test, model_name=\"iterative\", set_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN $1.05\\\\pm3.23$\n",
      "Barycenter $1.27\\\\pm3.90$\n",
      "mean $1.01\\\\pm3.10$\n",
      "MICE $1.02\\\\pm3.13$\n",
      "Dropout Spatial Only $4.29\\\\pm10.52$\n",
      "dropout $1.02\\\\pm0.63$\n",
      "Chained Spatial Only $4.29\\\\pm10.52$\n",
      "iterative $1.02\\\\pm0.63$\n"
     ]
    }
   ],
   "source": [
    "loss_knn_val = losses.compute_mse_loss(out_knn_test, target_missing_test, bold_missing_with_mask_test, model_name=\"kNN\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_barycenter_val = losses.compute_mse_loss(out_barycenter_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Barycenter\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_mean_val = losses.compute_mse_loss(out_mean_test, target_missing_test, bold_missing_with_mask_test, model_name=\"mean\", set_name=\"test\", root=True)\n",
    "\n",
    "#loss_ce_val = losses.compute_mse_loss(out_ce_test, target_missing_test, bold_missing_with_mask_test, model_name=\"ce\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_mice_val = losses.compute_mse_loss(out_mice_test, target_missing_test, bold_missing_with_mask_test, model_name=\"MICE\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_dropout_spatial_val = losses.compute_mse_loss(out_dropout_spatial_only_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Dropout Spatial Only\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_dropout_val = losses.compute_mse_loss_maskless(dropout_denoised_test, dropout_target_test, model_name=\"dropout\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_chained_spatial_val = losses.compute_mse_loss(out_chained_spatial_only_test, target_missing_test, bold_missing_with_mask_test, model_name=\"Chained Spatial Only\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_iterative_val = losses.compute_mse_loss_maskless(chained_denoised_test, chained_target_test, model_name=\"iterative\", set_name=\"test\", root=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputted_knn_bold_test, target_knn_test = data_utils.format_to_denoiser(out_knn_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_barycenter_bold_test, target_barycenter_test = data_utils.format_to_denoiser(out_barycenter_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_mean_bold_test, target_mean_test = data_utils.format_to_denoiser(out_mean_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "#imputted_ce_bold_test, target_ce_test = data_utils.format_to_denoiser(out_ce_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_mice_bold_test, target_mice_test = data_utils.format_to_denoiser(out_mice_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_dropout_spatial_only_bold_test, target_ce_test = data_utils.format_to_denoiser(out_dropout_spatial_only_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_chained_spatial_only_bold_test, target_ce_test = data_utils.format_to_denoiser(out_chained_spatial_only_test, denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_dropout_bold_test, target_dropout_test = data_utils.format_to_denoiser(data_utils.out_by_batch(dropout, denoising_bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels), denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)\n",
    "imputted_iterative_bold_test, target_iterative_test = data_utils.format_to_denoiser(data_utils.out_by_batch(iterative, denoising_bold_missing_with_mask_test, batch_size=current_batch_size, n_voxels=n_voxels), denoising_bold_missing_with_mask_test, target_bold_test, rm_rate=rm_rate, n_voxels=n_voxels, n_partitions=n_partitions, n_individuals=n_individuals_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN $1.10\\\\pm0.84$\n",
      "Barycenter $1.12\\\\pm0.85$\n",
      "mean $1.10\\\\pm0.84$\n",
      "MICE $1.10\\\\pm0.84$\n",
      "Dropout Spatial Only $1.09\\\\pm0.84$\n",
      "dropout $1.10\\\\pm0.82$\n",
      "Chained Spatial Only $1.09\\\\pm0.84$\n",
      "iterative $1.10\\\\pm0.82$\n"
     ]
    }
   ],
   "source": [
    "loss_knn_val = losses.compute_mae_loss_maskless(imputted_knn_bold_test, target_knn_test, model_name=\"kNN\", set_name=\"test\")\n",
    "\n",
    "loss_barycenter_val = losses.compute_mae_loss_maskless(imputted_barycenter_bold_test, target_barycenter_test, model_name=\"Barycenter\", set_name=\"test\")\n",
    "\n",
    "loss_mean_val = losses.compute_mae_loss_maskless(imputted_mean_bold_test, target_mean_test, model_name=\"mean\", set_name=\"test\")\n",
    "\n",
    "#loss_ce_val = losses.compute_mae_loss_maskless(imputted_ce_bold_test, target_ce_test, model_name=\"ce\", set_name=\"test\")\n",
    "\n",
    "loss_mice_val = losses.compute_mae_loss_maskless(imputted_mice_bold_test, target_mice_test, model_name=\"MICE\", set_name=\"test\")\n",
    "\n",
    "loss_dropout_spatial_only_val = losses.compute_mae_loss_maskless(imputted_dropout_spatial_only_bold_test, target_ce_test, model_name=\"Dropout Spatial Only\", set_name=\"test\")\n",
    "\n",
    "loss_dropout_val = losses.compute_mae_loss_maskless(imputted_dropout_bold_test, target_dropout_test, model_name=\"dropout\", set_name=\"test\")\n",
    "\n",
    "loss_chained_spatial_only_val = losses.compute_mae_loss_maskless(imputted_chained_spatial_only_bold_test, target_ce_test, model_name=\"Chained Spatial Only\", set_name=\"test\")\n",
    "\n",
    "loss_iterative_val = losses.compute_mae_loss_maskless(imputted_iterative_bold_test, target_iterative_test, model_name=\"iterative\", set_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN $1.39\\\\pm0.89$\n",
      "Barycenter $1.40\\\\pm0.89$\n",
      "mean $1.38\\\\pm0.89$\n",
      "MICE $1.38\\\\pm0.88$\n",
      "Dropout Spatial Only $1.37\\\\pm0.88$\n",
      "dropout $1.37\\\\pm0.87$\n",
      "Chained Spatial Only $1.37\\\\pm0.88$\n",
      "iterative $1.37\\\\pm0.87$\n"
     ]
    }
   ],
   "source": [
    "loss_knn_val = losses.compute_mse_loss_maskless(imputted_knn_bold_test, target_knn_test, model_name=\"kNN\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_barycenter_val = losses.compute_mse_loss_maskless(imputted_barycenter_bold_test, target_barycenter_test, model_name=\"Barycenter\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_mean_val = losses.compute_mse_loss_maskless(imputted_mean_bold_test, target_mean_test, model_name=\"mean\", set_name=\"test\", root=True)\n",
    "\n",
    "#loss_ce_val = losses.compute_mse_loss_maskless(imputted_ce_bold_test, target_ce_test, model_name=\"ce\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_mice_val = losses.compute_mse_loss_maskless(imputted_mice_bold_test, target_mice_test, model_name=\"MICE\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_dropout_spatial_only_val = losses.compute_mse_loss_maskless(imputted_dropout_spatial_only_bold_test, target_ce_test, model_name=\"Dropout Spatial Only\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_dropout_val = losses.compute_mse_loss_maskless(imputted_dropout_bold_test, target_dropout_test, model_name=\"dropout\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_chained_spatial_only_val = losses.compute_mse_loss_maskless(imputted_chained_spatial_only_bold_test, target_ce_test, model_name=\"Chained Spatial Only\", set_name=\"test\", root=True)\n",
    "\n",
    "loss_iterative_val = losses.compute_mse_loss_maskless(imputted_iterative_bold_test, target_iterative_test, model_name=\"iterative\", set_name=\"test\", root=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
